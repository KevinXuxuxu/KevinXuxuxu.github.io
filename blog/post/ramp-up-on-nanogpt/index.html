<!DOCTYPE html>
<html lang="en">
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1" />

        <link id="spectre" rel="stylesheet" href="/static/style/spectre.css">
        <link rel="stylesheet" href="/static/style/pygments_style.css">

        <script src="/static/script/color_theme.js"></script>
        <script>
            init_color_theme();

            // Manual responsive for blog images
            document.addEventListener("DOMContentLoaded", function () {
                if (window.innerWidth < 768) {
                    const elements = document.getElementsByClassName("my-resp-img");
                    for (var i=0; i<elements.length; i++) {
                        elements[i].style.width = "90%";
                    }
                }
            });
        </script>

        <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
    <title>Ramp Up on NanoGPT</title>

    <style>
        /* Initially hide the element */
        .hidden {
          display: none;
        }
        
        /* Show the element when it's hovered over */
        h3:hover .hidden {
          display: inline-block;
        }
        h4:hover .hidden {
          display: inline-block;
        }
        h5:hover .hidden {
          display: inline-block;
        }
    </style>

        <link rel="apple-touch-icon" sizes="180x180" href="/static/favicon_io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/static/favicon_io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/static/favicon_io/favicon-16x16.png">
        <link rel="manifest" href="/static/favicon_io/site.webmanifest">
    </head>
    <body>
        <div class="main container grid-lg">
            <header class="navbar">
                <section class="navbar-section">
                </section>
                <section class="navbar-section">
                    <label id="color-switch" class="form-switch">
                        <input id="theme-checkbox" type="checkbox" onclick="switch_color_theme();" disabled>
                        <i class="form-icon"></i> <p id="moon">⏳</p>
                    </label>
                </section>
            </header>
            <header class="column col-sm-12 col-md-10 col-8">
                
    <figure class="avatar avatar-sm">
        <img src="/static/favicon_io/android-chrome-192x192.png" alt="avatar">
    </figure>&nbsp;
    <a href="/">fzxu's Blog</a>
    <h1 class="heading-index">Ramp Up on NanoGPT</h1>

    <p class="text-gray">
        <small>2023-02-04 20:58:32</small>
        <span class="chip bg-secondary"><a href="/blog/category/tech/" class="text-primary">tech</a></span>
        
        <span class="chip bg-gray"><a href="/blog/tag/GPT/" class="text-dark">GPT</a></span>
        
        <span class="chip bg-gray"><a href="/blog/tag/LLM/" class="text-dark">LLM</a></span>
        
        <span class="chip bg-gray"><a href="/blog/tag/nanoGPT/" class="text-dark">nanoGPT</a></span>
        
        <span class="chip bg-gray"><a href="/blog/tag/ML/" class="text-dark">ML</a></span>
        
        <span class="chip bg-gray"><a href="/blog/tag/AI/" class="text-dark">AI</a></span>
        
    </p>

            </header>
                
            <main class="content columns">
                <div class="column col-sm-12 col-md-10 col-8">
                    
    
    <p>During Thanksgiving last year, I finally convinced myself to buy compartments (while the sales were still on) to utilize my NVIDIA RTX 3070 graphics card and assemble a new PC. I did it the first time in 2021 when I was still in Beijing, so this time it went pretty smoothly. Although the main motivation is to play PC games (Got all achievements in Elden Ring soon after :), I do hope to use the graphics card to do some deep learning experiments. So after <a href="https://karpathy.ai/">Andrej Karpathy</a> released <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a> and the <a href="https://karpathy.ai/zero-to-hero.html">Neural Networks: Zero to Hero</a> online course, I know it's a perfect project to start with.</p>
<h4 id="setup_environment">Setup environment&nbsp;<a class="hidden" tabindex="-1" href="#setup_environment" style="font-size: .8rem"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 8 8"><path fill="currentColor" d="M5.88.03a1.9 1.9 0 0 0-.53.09c-.27.1-.53.25-.75.47a.5.5 0 1 0 .69.69c.11-.11.24-.17.38-.22c.35-.12.78-.07 1.06.22c.39.39.39 1.04 0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47c-.26-.01-.41-.13-.41-.13a.5.5 0 1 0-.5.88s.34.22.84.25c.5.03 1.2-.16 1.81-.78l1.5-1.5A1.98 1.98 0 0 0 6.44.07C6.26.03 6.06.03 5.88.04zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59a1.98 1.98 0 0 0 0 2.81c.56.56 1.36.72 2.06.47c.27-.1.53-.25.75-.47a.5.5 0 1 0-.69-.69c-.11.11-.24.17-.38.22c-.35.12-.78.07-1.06-.22c-.39-.39-.39-1.04 0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44c.28.01.47.09.47.09a.5.5 0 1 0 .44-.88s-.34-.2-.84-.22z"/></svg></a></h4>
<p>As described in the nanoGPT README, it requires the following to be installed (not sure what the <code>&lt;3</code> means though)</p>
<ul>
<li><a href="https://pytorch.org">pytorch</a> &lt;3</li>
<li><a href="https://numpy.org/install/">numpy</a> &lt;3</li>
<li><code>pip install transformers</code> for huggingface transformers &lt;3 (to load GPT-2 checkpoints)</li>
<li><code>pip install datasets</code> for huggingface datasets &lt;3 (if you want to download + preprocess OpenWebText)</li>
<li><code>pip install tiktoken</code> for OpenAI's fast BPE code &lt;3</li>
<li><code>pip install wandb</code> for optional logging &lt;3</li>
<li><code>pip install tqdm</code></li>
</ul>
<p>While here's some caveats I encountered that worth some attention:</p>
<ul>
<li>For a fresh linux machine with just the graphics card driver, you need to install the CUDA libs and dependencies, and this should be on the OS itself, now within one conda environment. Follow this instruction and it might work: <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=deb_local">https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=deb_local</a></li>
<li>You need to use the nightly version of Pytorch to be able to <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">compile model</a> which is used by nanoGPT. So for my case use this to install pytorch:</li>
</ul>
<div class="highlight">
    <pre class="code" data-lang="shell"><span></span><code>conda install pytorch torchvision torchaudio pytorch-cuda<span class="o">=</span><span class="m">11</span>.8 <span class="se">\</span>
  -c pytorch-nightly -c nvidia
</code></pre>
</div><h4 id="training">Training&nbsp;<a class="hidden" tabindex="-1" href="#training" style="font-size: .8rem"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 8 8"><path fill="currentColor" d="M5.88.03a1.9 1.9 0 0 0-.53.09c-.27.1-.53.25-.75.47a.5.5 0 1 0 .69.69c.11-.11.24-.17.38-.22c.35-.12.78-.07 1.06.22c.39.39.39 1.04 0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47c-.26-.01-.41-.13-.41-.13a.5.5 0 1 0-.5.88s.34.22.84.25c.5.03 1.2-.16 1.81-.78l1.5-1.5A1.98 1.98 0 0 0 6.44.07C6.26.03 6.06.03 5.88.04zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59a1.98 1.98 0 0 0 0 2.81c.56.56 1.36.72 2.06.47c.27-.1.53-.25.75-.47a.5.5 0 1 0-.69-.69c-.11.11-.24.17-.38.22c-.35.12-.78.07-1.06-.22c-.39-.39-.39-1.04 0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44c.28.01.47.09.47.09a.5.5 0 1 0 .44-.88s-.34-.2-.84-.22z"/></svg></a></h4>
<p>Use the <code>prepare.py</code> script in each folder of <code>data/</code> to download and prepare the data into a binary file which will be directly used for training and validation. The prepare time mainly depend on which dataset you're using (open web text is pretty large) and your network speed.</p>
<p>The training part is also pretty straight forward. Most parameters I have touched are selecting data set, and the ones that controls the model size and data batch size:</p>
<div class="highlight">
    <pre class="code" data-lang="shell"><span></span><code>$ python train.py --dataset<span class="o">=</span>openwebtext <span class="se">\ </span> <span class="c1"># dataset to use</span>
    --n_layer<span class="o">=</span><span class="m">10</span> --n_head<span class="o">=</span><span class="m">10</span> --n_embd<span class="o">=</span><span class="m">500</span> <span class="se">\ </span> <span class="c1"># parameters to determine model size</span>
    --block_size<span class="o">=</span><span class="m">256</span> --batch_size<span class="o">=</span><span class="m">12</span>  <span class="c1"># parameters to determine block/batch size</span>
Overriding: <span class="nv">dataset</span> <span class="o">=</span> openwebtext
Overriding: <span class="nv">n_layer</span> <span class="o">=</span> <span class="m">10</span>
Overriding: <span class="nv">n_head</span> <span class="o">=</span> <span class="m">10</span>
Overriding: <span class="nv">n_embd</span> <span class="o">=</span> <span class="m">500</span>
Overriding: <span class="nv">block_size</span> <span class="o">=</span> <span class="m">256</span>
Overriding: <span class="nv">batch_size</span> <span class="o">=</span> <span class="m">12</span>
vocab_size not found <span class="k">in</span> data/openwebtext/meta.pkl, using GPT-2 default of <span class="m">50257</span>
Initializing a new model from scratch
number of parameters: <span class="m">55</span>.32M  <span class="c1"># &lt;---</span>
compiling the model... <span class="o">(</span>takes a ~minute<span class="o">)</span>
step <span class="m">0</span>: train loss <span class="m">10</span>.9285, val loss <span class="m">10</span>.9275
iter <span class="m">0</span>: loss <span class="m">10</span>.9285, <span class="nb">time</span> <span class="m">33950</span>.10ms
iter <span class="m">1</span>: loss <span class="m">10</span>.9264, <span class="nb">time</span> <span class="m">356</span>.75ms
iter <span class="m">2</span>: loss <span class="m">10</span>.9129, <span class="nb">time</span> <span class="m">367</span>.66ms
...
</code></pre>
</div><h4 id="parameter_tuning">Parameter Tuning&nbsp;<a class="hidden" tabindex="-1" href="#parameter_tuning" style="font-size: .8rem"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 8 8"><path fill="currentColor" d="M5.88.03a1.9 1.9 0 0 0-.53.09c-.27.1-.53.25-.75.47a.5.5 0 1 0 .69.69c.11-.11.24-.17.38-.22c.35-.12.78-.07 1.06.22c.39.39.39 1.04 0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47c-.26-.01-.41-.13-.41-.13a.5.5 0 1 0-.5.88s.34.22.84.25c.5.03 1.2-.16 1.81-.78l1.5-1.5A1.98 1.98 0 0 0 6.44.07C6.26.03 6.06.03 5.88.04zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59a1.98 1.98 0 0 0 0 2.81c.56.56 1.36.72 2.06.47c.27-.1.53-.25.75-.47a.5.5 0 1 0-.69-.69c-.11.11-.24.17-.38.22c-.35.12-.78.07-1.06-.22c-.39-.39-.39-1.04 0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44c.28.01.47.09.47.09a.5.5 0 1 0 .44-.88s-.34-.2-.84-.22z"/></svg></a></h4>
<p>I'm not going to talk about some fancy tuning since I'm a noob. The parameter tuning I was doing mostly focused on a more practical sense: to get the training running with limited resources (mostly VRAM). My 3070 comes with around 8G of VRAM and the training get to use around 6G practically speaking, which is not a lot. So I need to tune the parameters (mostly those determines the model size and those determines the data size feed into the model) to get the good balance between memory usage and model quality.</p>
<p>I did some experiments to get the stable training VRAM usage with different set of parameters, and here's the result (Data points (n_layer, n_head, n_embd), n_params, (block_size, batch_size) -&gt; stable VRAM usage during training)</p>
<ul>
<li>(8, 8, 128), 8.08M, (64, 12) -&gt; 1938M</li>
<li>(8, 8, 128), 8.08M, (128, 12) -&gt; 2424M</li>
<li>(8, 8, 128), 8.08M, (256, 12) -&gt; 3924M</li>
<li>(8, 8, 128), 8.08M, (512, 12) -&gt; 6836M (might not reproduce as it approach my gpu limit)</li>
<li>(10, 10, 500), 50.32M, (256, 12) -&gt; 5760M
Note that the dataset used for the above experiments are all open web test, as the shakespeare dataset is too small.</li>
</ul>
<p>If we assume <code>x</code> is the number of parameters (in millions), <code>y</code> is the number of datapoint in a batch (block_size * batch_size) and <code>z</code> is the VRAM it takes to train (in MB), here is a very simple fit from my data:</p>
<div class="highlight">
    <pre class="code" data-lang="Python"><span></span><code><span class="mf">42.65</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.93</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">766.86</span> <span class="o">=</span> <span class="n">z</span>
</code></pre>
</div><p>(BTW the python code I used to fit this data is generated by chatGPT! I'm doing other interesting things on that idea and will definitely write about it later.)</p>
<h4 id="result_evaluation">Result Evaluation&nbsp;<a class="hidden" tabindex="-1" href="#result_evaluation" style="font-size: .8rem"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 8 8"><path fill="currentColor" d="M5.88.03a1.9 1.9 0 0 0-.53.09c-.27.1-.53.25-.75.47a.5.5 0 1 0 .69.69c.11-.11.24-.17.38-.22c.35-.12.78-.07 1.06.22c.39.39.39 1.04 0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47c-.26-.01-.41-.13-.41-.13a.5.5 0 1 0-.5.88s.34.22.84.25c.5.03 1.2-.16 1.81-.78l1.5-1.5A1.98 1.98 0 0 0 6.44.07C6.26.03 6.06.03 5.88.04zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59a1.98 1.98 0 0 0 0 2.81c.56.56 1.36.72 2.06.47c.27-.1.53-.25.75-.47a.5.5 0 1 0-.69-.69c-.11.11-.24.17-.38.22c-.35.12-.78.07-1.06-.22c-.39-.39-.39-1.04 0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44c.28.01.47.09.47.09a.5.5 0 1 0 .44-.88s-.34-.2-.84-.22z"/></svg></a></h4>
<p>I wrote a <a href="https://github.com/KevinXuxuxu/nanoGPT/blob/master/inter_prompt.py">interactive prompt script</a> which is modified from Karpathy's original <a href="https://github.com/karpathy/nanoGPT/blob/master/sample.py">sample script</a>. THe interactive prompt provides a more user-friendly evaluation experience. Here are some result from the 2 dataset I trained on:</p>
<ul>
<li>Shakespeare dataset, 3.42M parameters, trained for an hours, didn't record final loss</li>
</ul>
<div class="highlight">
    <pre class="code" data-lang="text"><span></span><code>prompt: ROMEO:
ROMEO:
O, daughter, for I have, I know not the wrong
About his house.

JULIET:

DUKE VINCENTIO:
Why, you should not have a tempted me him.

LUCIO:
Very well, and advise him.

DUKE VINCENTIO:
What, are you?

VIRGILIA:
Ay, my gracious sir!

CORIOLANUS
---------------
prompt: MACBETH:
MACBETH:
Know, to be a prisoner.

Th, I shall be another thing.

KING RICHARD III:
Well, let them stay.

QUEEN ELIZABETH:
So must I learn.

KING RICHARD III:
I would beg a happy mother by the world;
Theseentss come in patience, and raise great danger;
And in this my hands, as many deep dear;
As oft as venomful as
---------------
prompt: To be, or not to be
To be, or not to be a kind something
That got the old man that seen thee sit:
So then, for humble, contem help, good counsel.

QUEEN MARGARET:
Not degenerate children, hold them not thyself.

LADY ANNE:
What, are thou that is it?

GLOUCESTER:
That is no traitor to the law, not garland
That he didhard heard his man that stoop
ToThat
</code></pre>
</div><p>The result does look like Shakespeare writing, except that it doesn't make sense for the most time. For example although we see Juliet show up in the result from &quot;ROMEO&quot; prompt, we also see e.g. DUKE VINCENTIO which is actually from &quot;Measure for Measure&quot; instead of &quot;Romeo and Juliet&quot;.</p>
<ul>
<li>Open web text dataset, 55.19M parameters, Trained for ~6h, final loss 3.9:</li>
</ul>
<div class="highlight">
    <pre class="code" data-lang="text"><span></span><code>prompt: Recipe for lasagna:
Recipe for lasagna:

(1) Spicy and piquadas
(2) Chocolate beans
(3) red beans
(4) egg whites
(5) cashews
(6) mozzarella
(6) soups
(8) basil
(9) paprika
(10) cumin
(11) cumin
(12) basil
(16) basil
(16) onions
---------------
prompt: More than 100 dead in Turkey and Syria. It’s just past 8am in
Gaziantep, Turkey, as we receive more information on the total number
of deaths cause by a powerful earthquake this morning.
More than 100 dead in Turkey and Syria. It’s just past 8am in Gaziantep,
Turkey, as we receive more information on the total number of deaths
cause by a powerful earthquake this morning.

The Syrian government has been accused of running down terrorists and
terrorists for the last year, the Associated Press reported.

The news outlet also claimed that Russian intelligence sources and
intelligence agencies have been sent to the Syrian government by state
and a foreign minister in the autonomous Syrian army, which has been
accused of bombing eastern Aleppo and the al-Aqsa hospital.

The Syrian government launched the effort on the city of Asm Talm, a
major port in the north of the city
---------------
prompt: In this section we begin to address the claim that TD methods
make more efficient use of their experience than do supervised-learning
methods, that they converge more rapidly and make more accurate
predictions along the way.
In this section we begin to address the claim that TD methods make
more efficient use of their experience than do supervised-learning
methods, that they converge more rapidly and make more accurate
predictions along the way. While the pattern of applying a pattern to
my-induced recall is pretty clear, that pattern is extremely important
in making sure that the results are consistent with my-induced recall.

For example, I wrote “FIFAN,” which includes the “real-world” and
“real-world” patterns. It’s an average f/A-level function to perform a
normal recall, but as such is generally the case when large-scale recall
---------------
</code></pre>
</div><p>Same as above, the result seems like normal writing, just doesn't make much sense. In the recipe prompt example, it want to put chocolate in lasagna, and there're 2 cumin and 3 basil in the list. In the international news example, it made up some places (e.g. al-Aqsa hospital, Asm Talm) that doesn't exist at all. For the last example, I picked the beginning of a paragraph from the famous <a href="http://incompleteideas.net/papers/sutton-88-with-erratum.pdf">Sutton 1988 RL paper</a>, but the generated content seems to be talking about some sort of information retrieval research?</p>
<p>Overall these models did well to generate content that resembles a certain kind of text, but did very poor when we look into details. I might get better results if I train for longer time to bring the loss under 2 (which I'll try sometime), and/or to train with more parameters which would require better hardware.</p>
<h4 id="future_plan">Future Plan&nbsp;<a class="hidden" tabindex="-1" href="#future_plan" style="font-size: .8rem"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 8 8"><path fill="currentColor" d="M5.88.03a1.9 1.9 0 0 0-.53.09c-.27.1-.53.25-.75.47a.5.5 0 1 0 .69.69c.11-.11.24-.17.38-.22c.35-.12.78-.07 1.06.22c.39.39.39 1.04 0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47c-.26-.01-.41-.13-.41-.13a.5.5 0 1 0-.5.88s.34.22.84.25c.5.03 1.2-.16 1.81-.78l1.5-1.5A1.98 1.98 0 0 0 6.44.07C6.26.03 6.06.03 5.88.04zm-2 2.31c-.5-.02-1.19.15-1.78.75L.6 4.59a1.98 1.98 0 0 0 0 2.81c.56.56 1.36.72 2.06.47c.27-.1.53-.25.75-.47a.5.5 0 1 0-.69-.69c-.11.11-.24.17-.38.22c-.35.12-.78.07-1.06-.22c-.39-.39-.39-1.04 0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44c.28.01.47.09.47.09a.5.5 0 1 0 .44-.88s-.34-.2-.84-.22z"/></svg></a></h4>
<p>Although the resulting model is hardly usable and close to garbage, I still find this experience exciting, since it utilized my graphics card and gives a good exposure to the latest and most popular technology of the day. I'm going to try to finish the online course by Karpathy and learn to fine tune the GPT model and try to train something useful with custom corpus.</p>

    
    <i><a class="text-secondary" style="font-size: .7rem;" href="https://github.com/KevinXuxuxu/blog/blob/main/posts/ramp-up-on-nanogpt.md">Markdown source</a></i>

    <script src="https://utteranc.es/client.js"
            repo="KevinXuxuxu/KevinXuxuxu.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>

                </div>
                <div class="column col-sm-12 col-md-2 col-4">
                    
                    
                </div>
            </main>
        </div>
    </body>
</html>